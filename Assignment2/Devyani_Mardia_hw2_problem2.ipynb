{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NAME:Devyani Mardia\n",
    "## NETID: dm1633\n",
    "\n",
    "################\n",
    "## Code for HW 2 problem 2\n",
    "##\n",
    "## INSTRUCTIONS:\n",
    "## The following file implements kernel SVM with\n",
    "## polynomial kernel.\n",
    "##\n",
    "## It also trains kernel SVM classifier on a\n",
    "## dataset that contains sociodemographic information of all counties\n",
    "## in the United States as well as how these counties voted in the 2016\n",
    "## presidential election.\n",
    "##\n",
    "## The kernel SVM classifier is use to predict whether a county prefers\n",
    "## Trump over Clinton based on sociodemographic features.\n",
    "##\n",
    "## Fill in the code in parts labeled \"FILL IN\".\n",
    "##\n",
    "################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trains kernel SVM via Projected Gradient Descent\n",
    "## and with polynomial kernel\n",
    "##\n",
    "## INPUT: \"X\" n--by--p matrix,\n",
    "##        \"Y\" n--by--1 vector of labels\n",
    "##        \"C\" scalar, weight placed on violation reduction\n",
    "##        \"d\" degree of polynomial kernel\n",
    "## OUTPUT:\n",
    "\n",
    "def kernelSVM(X, Y, C, d):\n",
    "    p = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "  \n",
    "    variables = 1e-5 * np.ones(n)\n",
    "\n",
    "    alpha = .1\n",
    "    gamma = .9\n",
    "    stepsize = 1\n",
    "  \n",
    "    K = (X @ X.T + 1)**d  \n",
    "    ## FILL IN: compute the kernel between all pairs of training samples\n",
    "    K2 = np.diag(Y) @ K @ np.diag(Y)\n",
    "  \n",
    "    it = 1\n",
    "    while True:\n",
    "        cur_obj = .5 * variables.T @ K2 @ variables - np.sum(variables)\n",
    "      \n",
    "        gradient = K2 @ variables - np.ones(n)\n",
    "        vars_new = dualproject(variables - stepsize * gradient, Y, C)\n",
    "    \n",
    "        ## backtracking line search\n",
    "        while (.5 * vars_new.T @ K2 @ vars_new - np.sum(vars_new) > \n",
    "               cur_obj + alpha * np.sum(gradient * (vars_new - variables))):\n",
    "            stepsize = stepsize * gamma\n",
    "            vars_new = dualproject(variables - stepsize * gradient, Y, C)\n",
    "    \n",
    "        new_obj = .5 * vars_new.T @ K2 @ vars_new - np.sum(vars_new)\n",
    "    \n",
    "        if it % 100 == 0:\n",
    "            print(\"Iteration: %d  objective: %.3f  conv threshold (log10) %.3f  stepsize %.3f\" % (\n",
    "                  it, new_obj, \n",
    "                  np.log10(np.mean((variables - vars_new)**2 / np.mean(variables**2))),\n",
    "                  stepsize\n",
    "                  ))\n",
    "    \n",
    "        it = it + 1\n",
    "        \n",
    "        if np.mean((variables - vars_new)**2) / np.mean(variables**2) < 1e-6:\n",
    "            break\n",
    "        else:\n",
    "            variables = vars_new\n",
    "  \n",
    "    return variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runs Dykstra's algorithm\n",
    "##\n",
    "## INPUT:  u is length-n vector\n",
    "##         Y is length-n vector\n",
    "##         C is a positive scalar\n",
    "##\n",
    "\n",
    "def dualproject(u, Y, C):\n",
    "    n = len(u)\n",
    "\n",
    "    v = u\n",
    "    p = np.zeros(n)\n",
    "    q = np.zeros(n)\n",
    "\n",
    "    T = 1000\n",
    "    for it in range(T):\n",
    "        w = project2(v + p, Y)\n",
    "        p = v + p - w\n",
    "        v = project1(w + q, C)\n",
    "        q = w + q - v\n",
    "\n",
    "        if (np.mean((v-w)**2) < 1e-20):\n",
    "            break\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def project1(u, C):\n",
    "    return np.minimum(np.maximum(u, 0), C)\n",
    "\n",
    "def project2(u, Y):\n",
    "    return u - Y * sum(u * Y)/sum(Y**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100  objective: -20.064  conv threshold (log10) -4.061  stepsize 0.001\n",
      "Iteration: 200  objective: -36.969  conv threshold (log10) -4.750  stepsize 0.001\n",
      "Iteration: 300  objective: -45.647  conv threshold (log10) -5.349  stepsize 0.001\n",
      "Iteration: 400  objective: -48.549  conv threshold (log10) -5.925  stepsize 0.001\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.088  LR Error: 0.091  #Support vec: 102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "##################\n",
    "##\n",
    "##  Using kernel SVM to predict voting behavior in the 2016 US presidential election.\n",
    "##\n",
    "######\n",
    "\n",
    "votes = pd.read_csv(\"votes.csv\")\n",
    "votes['prefer_trump'] = votes['trump'] > votes['clinton']\n",
    "features = [\"white\", \"black\", \"poverty\", \"density\", \"bachelor\", \"highschool\", \"age65plus\", \"income\", \"age18under\", \"population2014\"]\n",
    "\n",
    "X = votes[features].values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "Y = [int(x) for x in votes['prefer_trump'].values]\n",
    "Y = np.array(Y)\n",
    "\n",
    "ntrain = 400 \n",
    "test_ix = np.random.choice(votes.index, size=votes.shape[0] - ntrain, replace=False)\n",
    "learn_ix = [x for x in range(votes.shape[0]) if x not in test_ix]\n",
    "\n",
    "Y[Y == 0] = -1\n",
    "\n",
    "X1 = X[learn_ix, ]\n",
    "Y1 = Y[learn_ix]\n",
    "\n",
    "X2 = X[test_ix, ]\n",
    "Y2 = Y[test_ix]\n",
    "\n",
    "p = X.shape[1]\n",
    "\n",
    "## Run SVM\n",
    "C = 0.5\n",
    "deg = 1 ## Modify this to 2, 3, 4, 5, and 6 for part (b)\n",
    "alpha = kernelSVM(X1, Y1, C, deg)\n",
    "\n",
    "ixs = np.where((alpha > 0) & (alpha < C))[0]\n",
    "\n",
    "b = np.mean(Y1[ixs].reshape(-1,1) - ((X1[ixs] @ X1.T + 1)**deg)@(np.multiply(alpha,Y1).reshape(-1,1)), axis = 0) # FILL IN\n",
    "\n",
    "Y2_svm = np.sign(((X2 @ X1.T + 1)**deg) @ np.multiply(alpha,Y1) + b)   ## FILL IN: compute the labels predicted by kernel SVM\n",
    "\n",
    "svm_error = np.mean(np.abs(Y2_svm - Y2))/2 ## because Y2 and Y2_svm are +1/-1\n",
    "num_supp_vec = len(ixs) ## FILL IN: compute the number of support vectors\n",
    "\n",
    "# Run logistic regression\n",
    "Y1[Y1 == -1] = 0\n",
    "Y2[Y2 == -1] = 0\n",
    "\n",
    "X1 = np.append(X1, np.ones((X1.shape[0], 1)), axis=1)\n",
    "X2 = np.append(X2, np.ones((X2.shape[0], 1)), axis=1)\n",
    "\n",
    "w_lr = LogisticRegression(penalty=\"none\", solver=\"lbfgs\", fit_intercept=False).fit(X1, Y1).coef_[0]\n",
    "\n",
    "Y2_lr = X2 @ w_lr >= 0.0\n",
    "lr_error = np.mean(np.abs(Y2_lr - Y2))\n",
    "\n",
    "baseline_error = 1.0 - np.mean(Y2) if np.mean(Y1) > 0.5 else np.mean(Y2)\n",
    "\n",
    "print(\"Baseline Error: %.3f  kernelSVM Error: %.3f  LR Error: %.3f  #Support vec: %d\" % (baseline_error, svm_error, lr_error, num_supp_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART B\n",
    "\n",
    "def runner(degree):\n",
    "    print(\n",
    "        f\"=====================Running SVM for Degree = {degree} =============================\"\n",
    "    )\n",
    "    np.random.seed(1)\n",
    "\n",
    "    ##################\n",
    "    ##\n",
    "    ##  Using kernel SVM to predict voting behavior in the 2016 US presidential election.\n",
    "    ##\n",
    "    ######\n",
    "\n",
    "    votes = pd.read_csv(\"votes.csv\")\n",
    "    votes[\"prefer_trump\"] = votes[\"trump\"] > votes[\"clinton\"]\n",
    "    features = [\n",
    "        \"white\",\n",
    "        \"black\",\n",
    "        \"poverty\",\n",
    "        \"density\",\n",
    "        \"bachelor\",\n",
    "        \"highschool\",\n",
    "        \"age65plus\",\n",
    "        \"income\",\n",
    "        \"age18under\",\n",
    "        \"population2014\",\n",
    "    ]\n",
    "\n",
    "    X = votes[features].values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    Y = [int(x) for x in votes[\"prefer_trump\"].values]\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    ntrain = 400\n",
    "    test_ix = np.random.choice(votes.index, size=votes.shape[0] - ntrain, replace=False)\n",
    "    learn_ix = [x for x in range(votes.shape[0]) if x not in test_ix]\n",
    "\n",
    "    Y[Y == 0] = -1\n",
    "\n",
    "    X1 = X[learn_ix,]\n",
    "    Y1 = Y[learn_ix]\n",
    "\n",
    "    X2 = X[test_ix,]\n",
    "    Y2 = Y[test_ix]\n",
    "\n",
    "    p = X.shape[1]\n",
    "\n",
    "    ## Run SVM\n",
    "    C = 0.5\n",
    "    deg = degree  ## Modify this to 2, 3, 4, 5, and 6 for part (b)\n",
    "    alpha = kernelSVM(X1, Y1, C, deg)\n",
    "\n",
    "    ixs = np.where((alpha > 0) & (alpha < C))[0]\n",
    "\n",
    "    b = np.mean(\n",
    "        Y1[ixs].reshape(-1, 1)\n",
    "        - ((X1[ixs] @ X1.T + 1) ** deg) @ (np.multiply(alpha, Y1).reshape(-1, 1)),\n",
    "        axis=0,\n",
    "    )  # FILL IN\n",
    "\n",
    "    Y2_svm = np.sign(\n",
    "        ((X2 @ X1.T + 1) ** deg) @ np.multiply(alpha, Y1) + b\n",
    "    )  ## FILL IN: compute the labels predicted by kernel SVM\n",
    "\n",
    "    svm_error = np.mean(np.abs(Y2_svm - Y2)) / 2  ## because Y2 and Y2_svm are +1/-1\n",
    "    num_supp_vec = len(ixs)  ## FILL IN: compute the number of support vectors\n",
    "\n",
    "    # Run logistic regression\n",
    "    Y1[Y1 == -1] = 0\n",
    "    Y2[Y2 == -1] = 0\n",
    "\n",
    "    X1 = np.append(X1, np.ones((X1.shape[0], 1)), axis=1)\n",
    "    X2 = np.append(X2, np.ones((X2.shape[0], 1)), axis=1)\n",
    "\n",
    "    w_lr = (\n",
    "        LogisticRegression(penalty=\"none\", solver=\"lbfgs\", fit_intercept=False)\n",
    "        .fit(X1, Y1)\n",
    "        .coef_[0]\n",
    "    )\n",
    "\n",
    "    Y2_lr = X2 @ w_lr >= 0.0\n",
    "    lr_error = np.mean(np.abs(Y2_lr - Y2))\n",
    "\n",
    "    baseline_error = 1.0 - np.mean(Y2) if np.mean(Y1) > 0.5 else np.mean(Y2)\n",
    "\n",
    "    print(\n",
    "        \"\\nBaseline Error: %.3f  kernelSVM Error: %.3f  LR Error: %.3f  #Support vec: %d\"\n",
    "        % (baseline_error, svm_error, lr_error, num_supp_vec)\n",
    "    )\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================Running SVM for Degree = 1 =============================\n",
      "Iteration: 100  objective: -20.064  conv threshold (log10) -4.061  stepsize 0.001\n",
      "Iteration: 200  objective: -36.969  conv threshold (log10) -4.750  stepsize 0.001\n",
      "Iteration: 300  objective: -45.647  conv threshold (log10) -5.349  stepsize 0.001\n",
      "Iteration: 400  objective: -48.549  conv threshold (log10) -5.925  stepsize 0.001\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.088  LR Error: 0.091  #Support vec: 102\n",
      "\n",
      "\n",
      "=====================Running SVM for Degree = 2 =============================\n",
      "Iteration: 100  objective: -1.022  conv threshold (log10) -4.066  stepsize 0.000\n",
      "Iteration: 200  objective: -1.827  conv threshold (log10) -4.654  stepsize 0.000\n",
      "Iteration: 300  objective: -2.583  conv threshold (log10) -5.003  stepsize 0.000\n",
      "Iteration: 400  objective: -3.306  conv threshold (log10) -5.253  stepsize 0.000\n",
      "Iteration: 500  objective: -4.006  conv threshold (log10) -5.446  stepsize 0.000\n",
      "Iteration: 600  objective: -4.689  conv threshold (log10) -5.602  stepsize 0.000\n",
      "Iteration: 700  objective: -5.360  conv threshold (log10) -5.734  stepsize 0.000\n",
      "Iteration: 800  objective: -6.020  conv threshold (log10) -5.850  stepsize 0.000\n",
      "Iteration: 900  objective: -6.669  conv threshold (log10) -5.953  stepsize 0.000\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.080  LR Error: 0.091  #Support vec: 135\n",
      "\n",
      "\n",
      "=====================Running SVM for Degree = 3 =============================\n",
      "Iteration: 100  objective: -0.039  conv threshold (log10) -4.278  stepsize 0.000\n",
      "Iteration: 200  objective: -0.063  conv threshold (log10) -4.779  stepsize 0.000\n",
      "Iteration: 300  objective: -0.084  conv threshold (log10) -5.101  stepsize 0.000\n",
      "Iteration: 400  objective: -0.104  conv threshold (log10) -5.332  stepsize 0.000\n",
      "Iteration: 500  objective: -0.123  conv threshold (log10) -5.514  stepsize 0.000\n",
      "Iteration: 600  objective: -0.142  conv threshold (log10) -5.664  stepsize 0.000\n",
      "Iteration: 700  objective: -0.160  conv threshold (log10) -5.794  stepsize 0.000\n",
      "Iteration: 800  objective: -0.177  conv threshold (log10) -5.907  stepsize 0.000\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.086  LR Error: 0.091  #Support vec: 235\n",
      "\n",
      "\n",
      "=====================Running SVM for Degree = 4 =============================\n",
      "Iteration: 100  objective: -0.002  conv threshold (log10) -4.901  stepsize 0.000\n",
      "Iteration: 200  objective: -0.002  conv threshold (log10) -5.173  stepsize 0.000\n",
      "Iteration: 300  objective: -0.003  conv threshold (log10) -5.384  stepsize 0.000\n",
      "Iteration: 400  objective: -0.004  conv threshold (log10) -5.558  stepsize 0.000\n",
      "Iteration: 500  objective: -0.004  conv threshold (log10) -5.700  stepsize 0.000\n",
      "Iteration: 600  objective: -0.005  conv threshold (log10) -5.823  stepsize 0.000\n",
      "Iteration: 700  objective: -0.005  conv threshold (log10) -5.930  stepsize 0.000\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.085  LR Error: 0.091  #Support vec: 336\n",
      "\n",
      "\n",
      "=====================Running SVM for Degree = 5 =============================\n",
      "Iteration: 100  objective: 0.004  conv threshold (log10) -5.701  stepsize 0.000\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.150  LR Error: 0.091  #Support vec: 372\n",
      "\n",
      "\n",
      "=====================Running SVM for Degree = 6 =============================\n",
      "Iteration: 100  objective: 0.170  conv threshold (log10) -5.893  stepsize 0.000\n",
      "\n",
      "Baseline Error: 0.154  kernelSVM Error: 0.143  LR Error: 0.091  #Support vec: 381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running for different values of degrees as asked in question:\n",
    "for d in [1,2,3,4,5,6]:\n",
    "    runner(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with the code with degree set to 1, then 2, 3, 4, 5, and 6. Report the predictive error as\n",
    "well as the number of support vectors in each of the cases.\n",
    "\n",
    "\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.088  LR Error: 0.091  #Support vec: 102\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.080  LR Error: 0.091  #Support vec: 135\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.086  LR Error: 0.091  #Support vec: 235\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.085  LR Error: 0.091  #Support vec: 336\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.150  LR Error: 0.091  #Support vec: 372\n",
    "Baseline Error: 0.154  kernelSVM Error: 0.143  LR Error: 0.091  #Support vec: 381\n",
    "\n",
    "Predictive error increases as the polynomial degree increases, this can be expected as the higher degrees of freedom of the curve it is more likely to overfit the data causing the error to increase at higher degrees. It is lowest at degree=2.\n",
    "\n",
    "\n",
    "How does the number of support vectors change with the degree of the polynomial kernel? Explain why.\n",
    "\n",
    "As the degree of freedom increases the number of support vectors to fit this data also increases to give us a more smoother and tightly fitted curve to the data , hence the error increases as well indicating overfitting."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
